{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e57689",
   "metadata": {},
   "source": [
    "# The Modelling Process\n",
    "By this point, we have discussed the concept of a statistical model and have seen how we can break the model down into its assumed population distribution, mean function and variance function. Together, this fully defines the vast majority of statistical models you will be working with on this course. However, we have yet to place this framework within the context of *how* you would actually use a statistical model in practice. As such, this last section is all about the general process of working with a statistical model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b81f2",
   "metadata": {},
   "source": [
    "\n",
    "## Model Parameters\n",
    "Before we look at the modelling process, it is first important to understand the concept of *parameters* within a parametric statistical model. We discussed parameters earlier in terms of probability distributions, but it is important to make sure this is clear within our more general modelling framework.\n",
    "\n",
    "The parameters of the model represent those quantities that are *unknown*. Typically, these are taken as constants that relate to the population distribution. For instance, take the simple regression model\n",
    "\n",
    "$$\n",
    "\\begin{array}{rlr}\n",
    "  y_{i}|x_{i1}                        &\\sim \\mathcal{N}\\left(\\mu_{i},\\sigma^{2}_{i}\\right)  & \\text{(Population distribution)} \\\\\n",
    "  E\\left(y_{i}|x_{i1}\\right)          &= \\mu_{i} = \\beta_{0} + \\beta_{1}x_{i1}              & \\text{(Mean function)} \\\\\n",
    "  \\text{Var}\\left(y_{i}|x_{i1}\\right) &= \\sigma^{2}_{i} = \\sigma^{2}                        & \\text{(Variance function)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "At the level of the population distribution, there are two parameters for every $i$th value of $y$ ($\\mu_{i}$, $\\sigma^{2}_{i}$). This implies that there are $2n$ parameters in this model ($\\mu_{1}, \\mu_{2}, \\dots, \\mu_{n}, \\sigma^{2}_{1}, \\sigma^{2}_{2}, \\dots, \\sigma^{2}_{n}$). However, the mean function clarifies that each value of $\\mu_{i}$ is actually a combination of two constants ($\\beta_{0},\\beta_{1}$) and the $i$th value of the predictor $x_{i1}$. Similarly, the variance function clarifies that each value of $\\sigma^{2}_{i}$ is actually the same number. As such, this model actually contains *three* parameters ($\\beta_{0}, \\beta_{1}, \\sigma^{2}$). These are the only unknown values in the model and capture everything we need to know about the random variable $y$. We can think of this as a *simplification* process, where the core information in the $n$ data points across both $y$ and $x$ can be captured by only 3 numbers. This can be clarified by simplifying the model to its standard form\n",
    "\n",
    "$$\n",
    "y_{i} \\sim \\mathcal{N}\\left(\\beta_{0} + \\beta_{1}x_{i1}, \\sigma^{2}\\right),\n",
    "$$\n",
    "\n",
    "where we can see that the only things we do not know about this population distribution are the values of $\\beta_{0}$, $\\beta_{1}$ and $\\sigma^{2}$. The important concept here is that the values of the parameters are of *primary interest* in terms of both *fitting* the model to a set of data, but also in terms of *interpretation* and *inference*. These parameters *define* the population distribution and, when the mean function depends upon other predictor variables, captures the relationship between these variables and our outcome. In effect, the parameters tell us how the mean and variance shift across different values of the predictors and thus tell us how the value of the outcome depends upon the value of the predictors. Thinking back to our general assumption about an experiment, the parameters tell us what $f(\\mathbf{x})$ is in $y = f(\\mathbf{x}) + \\epsilon$. As such, in order to understand this relationship, we want to know what the parameter values are.\n",
    "\n",
    "Of importance here is that the parameters are defined at the level of the *population*. As such, we cannot know what they are unless we have the whole population at our disposal. Given that this is generally wildly impractical, we typically only have a *sample* from the population. This means that we can only *estimate* the values of the parameters. From this perspective, our general aim with any statistical model is to estimate the values of the population parameters. If these are good estimates, we hope to then be able to infer something about the population under study, taking into account both the values of the parameters, as well the uncertainty inherent in estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20771e5b",
   "metadata": {},
   "source": [
    "## The Steps of Modelling\n",
    "\n",
    "```{figure} images/modelling-complete-v.png\n",
    "---\n",
    "scale: 45%\n",
    "align: right\n",
    "name: model-flowchart-fig\n",
    "---\n",
    "Flowchart showing the general steps in building a statistical model.\n",
    "```\n",
    "\n",
    "The general steps in building a model are shown in {numref}`model-flowchart-fig`. As we can see, the process can be split into three steps. The first is *model specification*, where the population distribution is decided, along with a suitable mean and variance function. The second is *model estimation*, where the parameters that define the mean and variance functions are estimated, using some data. The outcome is a set of estimated values for the parameters, as well as some metric of the uncertainty of the estimation process. The final step is *inference*, where the parameter estimates and their uncertainty are combined to create *test-statistics* which are used to reach conclusions on the basis of *hypothesis* tests about the population values of the parameters.  \n",
    "\n",
    "Given the steps shown in {numref}`model-flowchart-fig`, this lesson has focussed primarily on *model specification*. In the next lesson, we will be looking at simple and multiple linear regression and will discuss both *model estimation* and *inference*. One particular reason for splitting these topics up is that model *specification* is a generic process that is applicable across many different contexts. For instance, both traditional statistical methods and Bayesian methods agree on this element of modelling. However, where methods disagree is usually in terms of the methods of *estimation* and *inference*. As such, it is useful to draw a conceptual line between these topics and just focus on *specification* for the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6996ab6",
   "metadata": {},
   "source": [
    "## Models in Theory vs Models in Practice\n",
    "As a final discussion before we finish, it is important to understand the distinction between the theoretical framework we have presented here and the reality of building statistical models. Although this framework appears very flexible, the reality is that we very rarely get to choose our own population distribution, mean function and variance function. Instead, we have a corpus of pre-specified models to choose from. This is largely because models need to be *tractable* and *practical*, particularly in terms of parameter estimation and statistical inference. As such, the framework we have presented is largely there for *understanding* the models that already exist, rather than as a framework for building brand new models. The degree of flexibility required to have full control over all the model elements is usually only possible within a Bayesian framework, wheres Frequentist methods are much more restrictive. As such, it is important to recognise that we will be working with models where the population distribution, mean function and variance function have already been decided for us. Our job is simply to make a choice in terms of which model will work best for us in any given situation. As we will come to learn, this encompasses regression, ANOVA, ANCOVA and a host of more complex methods. This framework is therefore helpful for understanding all the ways that these approaches differ, as wel as, most importantly, all the ways that they are *the same*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8565e44",
   "metadata": {},
   "source": [
    "`````{topic} What do you now know?\n",
    "In this section, we have explored the *modelling process* used to build statistical models during a data analysis. After reading this section, you should have a good sense of:\n",
    "\n",
    "- What the *parameters* of a model are and how they can be used to *summarise* and *simplify* the relationships within a data set.\n",
    "- The idea that the model parameters are *of most interest* in terms of their values, both for *interpretation* and *inference*.\n",
    "- The steps involved in building a model, specifically in terms of *specification*, *estimation* and *inference*.\n",
    "- The notion that most statistical methods are the *same* when it comes to model *specification*, but that differences come from both *estimation* and approaches to *inference*.\n",
    "- The fact that, in practice, we rarely build models up by fully specifying the population distribution, mean function and variance function. Instead, we used pre-existing models where these elements have already been fixed in order to make sure a model is *tractable* and *practical*. For instance, simply linear regression pre-supposed a normal population distribution, with a mean function given by the regression equation and a constant variance.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dfe84b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}